{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-22 14:56:24.251010: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import logging\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import umap\n",
    "from lavis.models.clip_models.loss import ClipLoss\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import random\n",
    "import itertools\n",
    "from sklearn.metrics import accuracy_score\n",
    "import argparse\n",
    "import ast\n",
    "\n",
    "import sys\n",
    "sys.path.append('/root/workspace/wht/multimodal_brain/src')\n",
    "from models.mlp import MLP,ProjectLayer,Direct\n",
    "from models.eeg import EEGEncoder,LSTMModel\n",
    "from models.ae import Autoencoder\n",
    "from dataset.things_eeg import EEGDataset\n",
    "\n",
    "from utils import set_seed,update_config,set_logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-22 14:56:29,951 - -------------------------START-------------------------\n",
      "2024-07-22 14:56:29,952 - CONFIG: {'data_dir': '/dev/shm/wht/datasets/things-eeg-small/Preprocessed_data_250Hz_whiten', 'exp_root': './exp', 'device': device(type='cuda', index=0), 'name': 'train_model_plot_subject', 'lr': 0.0001, 'epochs': 24, 'batch_size': 256, 'model_type': 'ViT-B-32', 'latend_dim': 512, 'logger': True, 'subjects': ['sub-01', 'sub-02', 'sub-03', 'sub-04', 'sub-05', 'sub-06', 'sub-07', 'sub-08', 'sub-09', 'sub-10'], 'model': {'eeg': {'name': 'Autoencoder', 'args': {'in_chans': 17}}, 'eeg_semantic': {'name': 'ProjectLayer', 'args': {'embedding_dim': 512, 'proj_dim': 512}}, 'aux': {'name': 'MLP', 'args': {'input_dim': 512, 'output_dim': 10, 'hiden_dims': []}}}, 'exp_dir': './exp/train_model_plot_subject'}\n",
      "2024-07-22 14:56:29,957 - ----load /dev/shm/wht/datasets/things-eeg-small/Preprocessed_data_250Hz_whiten/sub-01/test.pt----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-22 14:56:38,177 - eeg: torch.Size([200, 17, 250])\n",
      "2024-07-22 14:56:38,181 - label: (200,)\n",
      "2024-07-22 14:56:38,182 - img: (200,)\n",
      "2024-07-22 14:56:38,182 - text: (200,)\n",
      "2024-07-22 14:56:38,183 - session: (200, 80)\n",
      "2024-07-22 14:56:38,184 - ----load /dev/shm/wht/datasets/things-eeg-small/Preprocessed_data_250Hz_whiten/sub-02/test.pt----\n",
      "2024-07-22 14:56:46,901 - eeg: torch.Size([200, 17, 250])\n",
      "2024-07-22 14:56:46,904 - label: (200,)\n",
      "2024-07-22 14:56:46,904 - img: (200,)\n",
      "2024-07-22 14:56:46,905 - text: (200,)\n",
      "2024-07-22 14:56:46,905 - session: (200, 80)\n",
      "2024-07-22 14:56:46,906 - ----load /dev/shm/wht/datasets/things-eeg-small/Preprocessed_data_250Hz_whiten/sub-03/test.pt----\n",
      "2024-07-22 14:56:55,648 - eeg: torch.Size([200, 17, 250])\n",
      "2024-07-22 14:56:55,652 - label: (200,)\n",
      "2024-07-22 14:56:55,653 - img: (200,)\n",
      "2024-07-22 14:56:55,653 - text: (200,)\n",
      "2024-07-22 14:56:55,654 - session: (200, 80)\n",
      "2024-07-22 14:56:55,654 - ----load /dev/shm/wht/datasets/things-eeg-small/Preprocessed_data_250Hz_whiten/sub-04/test.pt----\n",
      "2024-07-22 14:57:04,463 - eeg: torch.Size([200, 17, 250])\n",
      "2024-07-22 14:57:04,464 - label: (200,)\n",
      "2024-07-22 14:57:04,464 - img: (200,)\n",
      "2024-07-22 14:57:04,465 - text: (200,)\n",
      "2024-07-22 14:57:04,465 - session: (200, 80)\n",
      "2024-07-22 14:57:04,466 - ----load /dev/shm/wht/datasets/things-eeg-small/Preprocessed_data_250Hz_whiten/sub-05/test.pt----\n",
      "2024-07-22 14:57:13,202 - eeg: torch.Size([200, 17, 250])\n",
      "2024-07-22 14:57:13,204 - label: (200,)\n",
      "2024-07-22 14:57:13,205 - img: (200,)\n",
      "2024-07-22 14:57:13,206 - text: (200,)\n",
      "2024-07-22 14:57:13,206 - session: (200, 80)\n",
      "2024-07-22 14:57:13,207 - ----load /dev/shm/wht/datasets/things-eeg-small/Preprocessed_data_250Hz_whiten/sub-06/test.pt----\n",
      "2024-07-22 14:57:21,911 - eeg: torch.Size([200, 17, 250])\n",
      "2024-07-22 14:57:21,912 - label: (200,)\n",
      "2024-07-22 14:57:21,912 - img: (200,)\n",
      "2024-07-22 14:57:21,912 - text: (200,)\n",
      "2024-07-22 14:57:21,913 - session: (200, 80)\n",
      "2024-07-22 14:57:21,913 - ----load /dev/shm/wht/datasets/things-eeg-small/Preprocessed_data_250Hz_whiten/sub-07/test.pt----\n",
      "2024-07-22 14:57:29,734 - eeg: torch.Size([200, 17, 250])\n",
      "2024-07-22 14:57:29,735 - label: (200,)\n",
      "2024-07-22 14:57:29,736 - img: (200,)\n",
      "2024-07-22 14:57:29,736 - text: (200,)\n",
      "2024-07-22 14:57:29,736 - session: (200, 80)\n",
      "2024-07-22 14:57:29,737 - ----load /dev/shm/wht/datasets/things-eeg-small/Preprocessed_data_250Hz_whiten/sub-08/test.pt----\n",
      "2024-07-22 14:57:33,007 - eeg: torch.Size([200, 17, 250])\n",
      "2024-07-22 14:57:33,009 - label: (200,)\n",
      "2024-07-22 14:57:33,009 - img: (200,)\n",
      "2024-07-22 14:57:33,010 - text: (200,)\n",
      "2024-07-22 14:57:33,011 - session: (200, 80)\n",
      "2024-07-22 14:57:33,011 - ----load /dev/shm/wht/datasets/things-eeg-small/Preprocessed_data_250Hz_whiten/sub-09/test.pt----\n",
      "2024-07-22 14:57:41,531 - eeg: torch.Size([200, 17, 250])\n",
      "2024-07-22 14:57:41,532 - label: (200,)\n",
      "2024-07-22 14:57:41,533 - img: (200,)\n",
      "2024-07-22 14:57:41,533 - text: (200,)\n",
      "2024-07-22 14:57:41,534 - session: (200, 80)\n",
      "2024-07-22 14:57:41,534 - ----load /dev/shm/wht/datasets/things-eeg-small/Preprocessed_data_250Hz_whiten/sub-10/test.pt----\n",
      "2024-07-22 14:57:46,534 - eeg: torch.Size([200, 17, 250])\n",
      "2024-07-22 14:57:46,535 - label: (200,)\n",
      "2024-07-22 14:57:46,536 - img: (200,)\n",
      "2024-07-22 14:57:46,537 - text: (200,)\n",
      "2024-07-22 14:57:46,537 - session: (200, 80)\n",
      "2024-07-22 14:57:52,987 - ----load /dev/shm/wht/datasets/things-eeg-small/Preprocessed_data_250Hz_whiten/sub-01/train.pt----\n",
      "2024-07-22 14:58:08,744 - eeg: torch.Size([66160, 17, 250])\n",
      "2024-07-22 14:58:08,747 - label: (66160,)\n",
      "2024-07-22 14:58:08,748 - img: (66160,)\n",
      "2024-07-22 14:58:08,749 - text: (66160,)\n",
      "2024-07-22 14:58:08,750 - session: (66160,)\n",
      "2024-07-22 14:58:08,750 - ----load /dev/shm/wht/datasets/things-eeg-small/Preprocessed_data_250Hz_whiten/sub-02/train.pt----\n",
      "2024-07-22 14:58:11,299 - eeg: torch.Size([66160, 17, 250])\n",
      "2024-07-22 14:58:11,300 - label: (66160,)\n",
      "2024-07-22 14:58:11,301 - img: (66160,)\n",
      "2024-07-22 14:58:11,301 - text: (66160,)\n",
      "2024-07-22 14:58:11,302 - session: (66160,)\n",
      "2024-07-22 14:58:11,303 - ----load /dev/shm/wht/datasets/things-eeg-small/Preprocessed_data_250Hz_whiten/sub-03/train.pt----\n",
      "2024-07-22 14:58:18,313 - eeg: torch.Size([66160, 17, 250])\n",
      "2024-07-22 14:58:18,316 - label: (66160,)\n",
      "2024-07-22 14:58:18,317 - img: (66160,)\n",
      "2024-07-22 14:58:18,317 - text: (66160,)\n",
      "2024-07-22 14:58:18,318 - session: (66160,)\n",
      "2024-07-22 14:58:18,319 - ----load /dev/shm/wht/datasets/things-eeg-small/Preprocessed_data_250Hz_whiten/sub-04/train.pt----\n",
      "2024-07-22 14:58:20,864 - eeg: torch.Size([66160, 17, 250])\n",
      "2024-07-22 14:58:20,865 - label: (66160,)\n",
      "2024-07-22 14:58:20,866 - img: (66160,)\n",
      "2024-07-22 14:58:20,867 - text: (66160,)\n",
      "2024-07-22 14:58:20,868 - session: (66160,)\n",
      "2024-07-22 14:58:20,869 - ----load /dev/shm/wht/datasets/things-eeg-small/Preprocessed_data_250Hz_whiten/sub-05/train.pt----\n",
      "2024-07-22 14:58:23,407 - eeg: torch.Size([66160, 17, 250])\n",
      "2024-07-22 14:58:23,413 - label: (66160,)\n",
      "2024-07-22 14:58:23,414 - img: (66160,)\n",
      "2024-07-22 14:58:23,415 - text: (66160,)\n",
      "2024-07-22 14:58:23,415 - session: (66160,)\n",
      "2024-07-22 14:58:23,416 - ----load /dev/shm/wht/datasets/things-eeg-small/Preprocessed_data_250Hz_whiten/sub-06/train.pt----\n",
      "2024-07-22 14:58:25,903 - eeg: torch.Size([66160, 17, 250])\n",
      "2024-07-22 14:58:25,905 - label: (66160,)\n",
      "2024-07-22 14:58:25,905 - img: (66160,)\n",
      "2024-07-22 14:58:25,906 - text: (66160,)\n",
      "2024-07-22 14:58:25,907 - session: (66160,)\n",
      "2024-07-22 14:58:25,908 - ----load /dev/shm/wht/datasets/things-eeg-small/Preprocessed_data_250Hz_whiten/sub-07/train.pt----\n",
      "2024-07-22 14:58:28,818 - eeg: torch.Size([66160, 17, 250])\n",
      "2024-07-22 14:58:28,823 - label: (66160,)\n",
      "2024-07-22 14:58:28,824 - img: (66160,)\n",
      "2024-07-22 14:58:28,825 - text: (66160,)\n",
      "2024-07-22 14:58:28,826 - session: (66160,)\n",
      "2024-07-22 14:58:28,827 - ----load /dev/shm/wht/datasets/things-eeg-small/Preprocessed_data_250Hz_whiten/sub-08/train.pt----\n",
      "2024-07-22 14:58:31,305 - eeg: torch.Size([66160, 17, 250])\n",
      "2024-07-22 14:58:31,306 - label: (66160,)\n",
      "2024-07-22 14:58:31,307 - img: (66160,)\n",
      "2024-07-22 14:58:31,308 - text: (66160,)\n",
      "2024-07-22 14:58:31,309 - session: (66160,)\n",
      "2024-07-22 14:58:31,310 - ----load /dev/shm/wht/datasets/things-eeg-small/Preprocessed_data_250Hz_whiten/sub-09/train.pt----\n",
      "2024-07-22 14:58:33,701 - eeg: torch.Size([66160, 17, 250])\n",
      "2024-07-22 14:58:33,702 - label: (66160,)\n",
      "2024-07-22 14:58:33,703 - img: (66160,)\n",
      "2024-07-22 14:58:33,704 - text: (66160,)\n",
      "2024-07-22 14:58:33,704 - session: (66160,)\n",
      "2024-07-22 14:58:33,705 - ----load /dev/shm/wht/datasets/things-eeg-small/Preprocessed_data_250Hz_whiten/sub-10/train.pt----\n",
      "2024-07-22 14:58:36,089 - eeg: torch.Size([66160, 17, 250])\n",
      "2024-07-22 14:58:36,091 - label: (66160,)\n",
      "2024-07-22 14:58:36,092 - img: (66160,)\n",
      "2024-07-22 14:58:36,092 - text: (66160,)\n",
      "2024-07-22 14:58:36,093 - session: (66160,)\n",
      "2024-07-22 14:58:37,279 - train num: 661600, test num: 2000\n"
     ]
    }
   ],
   "source": [
    "set_seed(0)\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "selected_ch = ['P7', 'P5', 'P3', 'P1','Pz', 'P2', 'P4', 'P6', 'P8', 'PO7', 'PO3', 'POz', 'PO4', 'PO8','O1', 'Oz', 'O2']\n",
    "model_type = 'ViT-B-32'\n",
    "latend_dim_dict = {'ViT-B-16':512,'ViT-B-32':512,'ViT-L-14': 768,'RN50':1024,'RN101':512,'RN50x4':640,'ViT-H-14':1024,'ViT-g-14':1024,'ViT-bigG-14':1280}\n",
    "latend_dim = latend_dim_dict[model_type]\n",
    "config = {\n",
    "    \"data_dir\": \"/dev/shm/wht/datasets/things-eeg-small/Preprocessed_data_250Hz_whiten\",#\"/dev/shm/wht/datasets/things-eeg-small/Preprocessed_data_250Hz/\",\n",
    "    \"exp_root\":'./exp',\n",
    "    \"device\":device,\n",
    "    \"name\": 'train_model_plot_subject',\n",
    "    \"lr\": 1e-4,\n",
    "    \"epochs\": 24,\n",
    "    \"batch_size\": 256,\n",
    "    \"model_type\":model_type,\n",
    "    \"latend_dim\":latend_dim,\n",
    "    \"logger\": True,\n",
    "    \"subjects\":['sub-01','sub-02','sub-03', 'sub-04', 'sub-05', 'sub-06', 'sub-07', 'sub-08', 'sub-09', 'sub-10'],\n",
    "    # \"eeg\":{'name':'ProjectLayer','args':{'embedding_dim':len(selected_ch)*250, 'proj_dim':latend_dim}},\n",
    "    \n",
    "    \"model\":{\"eeg\":{'name':'Autoencoder','args':{'in_chans':len(selected_ch)}},\n",
    "                \"eeg_semantic\":{'name':'ProjectLayer','args':{'embedding_dim':latend_dim, 'proj_dim':latend_dim}},\n",
    "                \"aux\":{'name':'MLP','args':{'input_dim':latend_dim_dict[model_type],'output_dim':10,'hiden_dims':[]}}},\n",
    "    # \"eeg\":{'name':'Autoencoder','args':{'in_chans':len(selected_ch)}},\n",
    "    # \"aux\":{'name':'MLP','args':{'input_dim':latend_dim_dict[model_type],'output_dim':10,'hiden_dims':[]}},\n",
    "    # \"aux2\":{'name':'MLP','args':{'input_dim':latend_dim_dict[model_type],'output_dim':10,'hiden_dims':[]}},\n",
    "}\n",
    "config['exp_dir'] = os.path.join(config['exp_root'],config['name'])\n",
    "\n",
    "os.makedirs(config['exp_dir'],exist_ok=True)\n",
    "\n",
    "set_logging(os.path.join(config['exp_dir'],f\"{'_'.join(config['subjects'])}.LOG\"))\n",
    "logging.info(f\"-------------------------START-------------------------\")\n",
    "logging.info(f\"CONFIG: {config}\")\n",
    "\n",
    "transform = transforms.Compose([\n",
    "])\n",
    "\n",
    "test_dataset = EEGDataset(data_dir=config['data_dir'],subjects=config['subjects'],model_type=config['model_type'],mode='test',selected_ch=selected_ch,transform=transform,avg=True)\n",
    "train_dataset = EEGDataset(data_dir=config['data_dir'],subjects=config['subjects'],model_type=config['model_type'],mode='train',selected_ch=selected_ch,transform=transform,avg=False)\n",
    "    \n",
    "logging.info(f\"train num: {len(train_dataset)}, test num: {len(test_dataset)}\")\n",
    "test_loader = DataLoader(test_dataset, batch_size=200, shuffle=False, drop_last=False,num_workers=4)\n",
    "train_loader = DataLoader(train_dataset, batch_size=config['batch_size'], shuffle=True, drop_last=False, num_workers=4, pin_memory=True)\n",
    "\n",
    "data={\n",
    "    'train':train_loader,\n",
    "    'test':test_loader\n",
    "}\n",
    "model = {k:globals()[v['name']](**v['args']).to(device) for k,v in config[\"model\"].items()}\n",
    "\n",
    "train_eval_args = {\n",
    "    'optimizer':optim.Adam([{'params': v.parameters(), 'lr': config['lr']} for k,v in model.items()]),\n",
    "    'criterion':ClipLoss(),\n",
    "    'epochs':config['epochs'],\n",
    "    'subjects':config['subjects'],\n",
    "    'best_acc':-1,\n",
    "    'best_epoch':-1,\n",
    "    'best_train_acc':-1,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.006 0.0425\n",
      "(2000, 512) (2000, 17, 250)\n",
      "0.02 0.0855\n",
      "(2000, 512) (2000, 17, 250)\n",
      "0.03 0.112\n",
      "(2000, 512) (2000, 17, 250)\n",
      "0.0425 0.154\n",
      "(2000, 512) (2000, 17, 250)\n",
      "0.0545 0.1895\n",
      "(2000, 512) (2000, 17, 250)\n",
      "0.0725 0.222\n",
      "(2000, 512) (2000, 17, 250)\n",
      "0.082 0.2555\n",
      "(2000, 512) (2000, 17, 250)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_119199/2351544741.py:96: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  plt.figure(figsize=(4, 4),dpi=300)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.11 0.3015\n",
      "(2000, 512) (2000, 17, 250)\n",
      "0.1235 0.356\n",
      "(2000, 512) (2000, 17, 250)\n",
      "0.159 0.4075\n",
      "(2000, 512) (2000, 17, 250)\n",
      "0.1745 0.412\n",
      "(2000, 512) (2000, 17, 250)\n",
      "0.1625 0.423\n",
      "(2000, 512) (2000, 17, 250)\n",
      "0.1705 0.416\n",
      "(2000, 512) (2000, 17, 250)\n",
      "0.154 0.3985\n",
      "(2000, 512) (2000, 17, 250)\n",
      "0.1775 0.438\n",
      "(2000, 512) (2000, 17, 250)\n",
      "0.177 0.4135\n",
      "(2000, 512) (2000, 17, 250)\n",
      "0.1575 0.395\n",
      "(2000, 512) (2000, 17, 250)\n",
      "0.1505 0.388\n",
      "(2000, 512) (2000, 17, 250)\n",
      "0.1465 0.3685\n",
      "(2000, 512) (2000, 17, 250)\n",
      "0.1475 0.359\n",
      "(2000, 512) (2000, 17, 250)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "exp_dir = 'SoftClipLoss_gan_transformer_lr1e-4_bs256'\n",
    "for epoch in range(25):\n",
    "    ckpt = torch.load(os.path.join('..',config['exp_root'],exp_dir,f'ckpt_{epoch}_sub-01_sub-02_sub-03_sub-04_sub-05_sub-06_sub-07_sub-08_sub-09_sub-10.pth'),map_location=config['device'])\n",
    "    for k,v in model.items():\n",
    "        v.load_state_dict(ckpt[k])\n",
    "        v.eval()\n",
    "\n",
    "    X = []\n",
    "    Y = []\n",
    "    S = []\n",
    "    Source_X = []\n",
    "    EEG =[]\n",
    "\n",
    "    all_predicted_classes = []\n",
    "    all_true_labels = []\n",
    "\n",
    "    all_text_features=data['test'].dataset.all_text_features\n",
    "    all_image_features=data['test'].dataset.all_image_features\n",
    "    all_text_features = all_text_features/all_text_features.norm(dim=-1, keepdim=True).to(device)\n",
    "    all_image_features = all_image_features/all_image_features.norm(dim=-1, keepdim=True).to(device)\n",
    "    for i,sample in enumerate(data['test']):\n",
    "        eeg, label, img, img_features,text, text_features ,session,subject = sample\n",
    "        eeg = eeg.to(device)\n",
    "        label = label.to(device)\n",
    "        \n",
    "        img_features = img_features.to(device)\n",
    "        text_features = text_features.to(device)\n",
    "        \n",
    "        # semantic_features = model['eeg'](eeg)\n",
    "        eeg_features = model['eeg'].forward_cls(eeg)\n",
    "        eeg_semantic = model['eeg_semantic'](eeg_features)\n",
    "        # semantic_logits = model['aux'](eeg_features)\n",
    "        # semantic_features = eeg_features[:,:512]\n",
    "        # bias_features = eeg_features[:,-512:]\n",
    "\n",
    "        eeg_semantic = eeg_semantic/eeg_semantic.norm(dim=-1, keepdim=True)\n",
    "        img_features = img_features/img_features.norm(dim=-1, keepdim=True)\n",
    "        \n",
    "        Source_X.append(eeg.detach().cpu().numpy())\n",
    "        EEG.append(eeg_features.detach().cpu().numpy())\n",
    "        X.append(eeg_semantic.detach().cpu().numpy())\n",
    "        Y.append(img_features.detach().cpu().numpy())\n",
    "        S.append(subject.detach().cpu().numpy())\n",
    "        \n",
    "        \n",
    "        similarity = (eeg_semantic @ all_image_features.T)\n",
    "        top_kvalues, top_k_indices = similarity.topk(5, dim=-1)\n",
    "        all_predicted_classes.append(top_k_indices.cpu().numpy())\n",
    "        all_true_labels.extend(label.cpu().numpy())\n",
    "    all_predicted_classes = np.concatenate(all_predicted_classes,axis=0)\n",
    "    all_true_labels = np.array(all_true_labels)\n",
    "\n",
    "    top_1_predictions = all_predicted_classes[:, 0]\n",
    "    top_1_correct = top_1_predictions == all_true_labels\n",
    "    top_1_accuracy = sum(top_1_correct)/len(top_1_correct)\n",
    "    top_k_correct = (all_predicted_classes == all_true_labels[:, np.newaxis]).any(axis=1)\n",
    "    top_k_accuracy = sum(top_k_correct)/len(top_k_correct)\n",
    "    print(top_1_accuracy,top_k_accuracy)\n",
    "    \n",
    "    EEG = np.concatenate(EEG,axis=0)\n",
    "    X = np.concatenate(X,axis=0)\n",
    "    Y = np.concatenate(Y,axis=0)\n",
    "    S = np.concatenate(S,axis=0)\n",
    "\n",
    "    SX = np.concatenate(Source_X,axis=0)\n",
    "    print(X.shape,SX.shape)\n",
    "    \n",
    "    reducer = umap.UMAP(n_neighbors=20, random_state=0)\n",
    "    embedding_2d = reducer.fit_transform(EEG)\n",
    "    plt.figure(figsize=(4, 4),dpi=300) \n",
    "    for label in set(S):\n",
    "        indices = S == label\n",
    "        plt.scatter(embedding_2d[indices, 0], embedding_2d[indices, 1],label=f'Subject {label}',s=1,alpha=1.0)\n",
    "    plt.legend(loc='best', markerscale=5, fontsize='xx-small')\n",
    "\n",
    "    os.makedirs(f'../results/{exp_dir}',exist_ok=True)\n",
    "    plt.savefig(f'../results/{exp_dir}/EEG_{epoch}.png', bbox_inches='tight')\n",
    "    \n",
    "    reducer = umap.UMAP(n_neighbors=20, random_state=0)\n",
    "    embedding_2d = reducer.fit_transform(X)\n",
    "    plt.figure(figsize=(4, 4),dpi=300) \n",
    "    for label in set(S):\n",
    "        indices = S == label\n",
    "        plt.scatter(embedding_2d[indices, 0], embedding_2d[indices, 1],label=f'Subject {label}',s=1,alpha=1.0)\n",
    "    plt.legend(loc='best', markerscale=5, fontsize='xx-small')\n",
    "\n",
    "    os.makedirs(f'../results/{exp_dir}',exist_ok=True)\n",
    "    plt.savefig(f'../results/{exp_dir}/Semantic_{epoch}.png', bbox_inches='tight')\n",
    "    \n",
    "    X_img = np.concatenate((X, all_image_features.cpu().numpy()), axis=0)\n",
    "    labels = np.array([0]*X.shape[0] + [1]*all_image_features.shape[0])\n",
    "    reducer = umap.UMAP(n_neighbors=20, random_state=0)\n",
    "    embedding_2d = reducer.fit_transform(X_img)\n",
    "    plt.figure(figsize=(4, 4),dpi=300)\n",
    "    for label in set(labels):\n",
    "        indices = labels == label\n",
    "        plt.scatter(embedding_2d[indices, 0], embedding_2d[indices, 1],label='Image' if label else 'EEG Semantic',s=1,alpha=1.0)\n",
    "    plt.legend(loc='best', markerscale=5, fontsize='xx-small')\n",
    "\n",
    "    os.makedirs(f'../results/{exp_dir}',exist_ok=True)\n",
    "    plt.savefig(f'../results/{exp_dir}/Image_Semantic_{epoch}.png', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "len(subject)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "SX = SX.reshape(2000,-1)\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=256)\n",
    "data_pca = pca.fit_transform(SX)\n",
    "reducer = umap.UMAP(n_neighbors=50, random_state=0)\n",
    "embedding_2d = reducer.fit_transform(data_pca)\n",
    "plt.figure(figsize=(6, 6),dpi=300) \n",
    "for label in set(S):\n",
    "    indices = S == label\n",
    "    plt.scatter(embedding_2d[indices, 0], embedding_2d[indices, 1],label=label,s=1)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "reducer = umap.UMAP(n_neighbors=20, random_state=0)\n",
    "embedding_2d = reducer.fit_transform(X)\n",
    "plt.figure(figsize=(4, 4),dpi=300) \n",
    "for label in set(S):\n",
    "    indices = S == label\n",
    "    plt.scatter(embedding_2d[indices, 0], embedding_2d[indices, 1],label=f'Subject {label}',s=1)\n",
    "plt.legend(loc='best', markerscale=5, fontsize='xx-small')\n",
    "\n",
    "plt.savefig('../results/mlp_final_ckpt.png', bbox_inches='tight')  # Save the figure with tight bounding box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "latent_np = latent.cpu().detach().numpy()\n",
    "distance_matrix = cdist(latent_np, latent_np, metric='euclidean')\n",
    "print(distance_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "average_distance = np.mean(distance_matrix)\n",
    "print(average_distance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "_ = []\n",
    "for i in range(10):\n",
    "    eeg = test_dataset.loaded_data[i]['eeg']\n",
    "    eeg = eeg.to(config['device']).to(torch.float32)\n",
    "    \n",
    "    latent = eeg_model(eeg) \n",
    "    \n",
    "    latent = latent.unsqueeze(0)\n",
    "    _.append(latent.cpu().detach().numpy())\n",
    "\n",
    "_ = np.concatenate(_)\n",
    "print(_.shape)\n",
    "\n",
    "avg = []\n",
    "for index in range(200):\n",
    "    l = _[:,index]\n",
    "    distance_matrix = cdist(l, l, metric='euclidean')\n",
    "    average_distance = np.mean(distance_matrix)\n",
    "    avg.append(average_distance)\n",
    "print(np.mean(avg))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
