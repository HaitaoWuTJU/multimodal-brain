{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "subject_list = [f'sub-{str(i).zfill(2)}' for i in range(1, 11)]\n",
    "\n",
    "all_list=[]\n",
    "\n",
    "eeg_list = []\n",
    "for subject in subject_list:\n",
    "    data_path= f'/dev/shm/wht/datasets/things-eeg-small/Preprocessed_data_250Hz_whiten/{subject}/test.pt'\n",
    "    loaded_data = torch.load(data_path)\n",
    "    loaded_data['eeg'] = torch.from_numpy(loaded_data['eeg'])\n",
    "    selected_ch = ['P7', 'P5', 'P3', 'P1','Pz', 'P2', 'P4', 'P6', 'P8', 'PO7', 'PO3', 'POz', 'PO4', 'PO8','O1', 'Oz', 'O2']\n",
    "    # selected_ch = None\n",
    "    channels = ['Fp1', 'Fp2', 'AF7', 'AF3', 'AFz', 'AF4', 'AF8', 'F7', 'F5', 'F3',\n",
    "                            'F1', 'F2', 'F4', 'F6', 'F8', 'FT9', 'FT7', 'FC5', 'FC3', 'FC1', \n",
    "                            'FCz', 'FC2', 'FC4', 'FC6', 'FT8', 'FT10', 'T7', 'C5', 'C3', 'C1',\n",
    "                            'Cz', 'C2', 'C4', 'C6', 'T8', 'TP9', 'TP7', 'CP5', 'CP3', 'CP1', \n",
    "                            'CPz', 'CP2', 'CP4', 'CP6', 'TP8', 'TP10', 'P7', 'P5', 'P3', 'P1',\n",
    "                            'Pz', 'P2', 'P4', 'P6', 'P8', 'PO7', 'PO3', 'POz', 'PO4', 'PO8',\n",
    "                            'O1', 'Oz', 'O2']\n",
    "    if selected_ch:\n",
    "        selected_idx = [channels.index(ch) for ch in selected_ch]\n",
    "        loaded_data['eeg'] = loaded_data['eeg'][:,:,selected_idx]\n",
    "    else:\n",
    "        selected_ch = channels\n",
    "    eeg = loaded_data['eeg'].to(torch.float32)\n",
    "    num_ch = len(selected_ch) if selected_ch else 63\n",
    "    eeg_mean = eeg.mean(axis=1)\n",
    "\n",
    "    eeg_list.append(eeg_mean)\n",
    "\n",
    "    norm_list =[]\n",
    "    for i in range(eeg.shape[0]):\n",
    "        diff_value = eeg[i] - eeg_mean[i]\n",
    "        norm_v = torch.norm(diff_value, 'fro')\n",
    "        norm_list.append(norm_v.item())\n",
    "    \n",
    "    all_list.append(norm_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [ele[:2] for ele in eeg_list]\n",
    "data = np.array(data)\n",
    "print(data.shape)\n",
    "data = data.reshape(10*2*1,-1)\n",
    "\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import umap\n",
    "n_components = 100\n",
    "n_neighbors = 50\n",
    "pca = PCA(n_components=n_components)\n",
    "data_pca = pca.fit_transform(data)\n",
    "reducer = umap.UMAP(n_neighbors=n_neighbors, random_state=0)\n",
    "embedding_2d = reducer.fit_transform(data_pca)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 8),dpi=800) \n",
    "n_subjects = 10 \n",
    "n_stimuli = 2\n",
    "n_samples_per_stimulus = 80\n",
    "subjects = np.repeat([f\"Subject {i+1}\" for i in range(n_subjects)], n_stimuli * n_samples_per_stimulus)\n",
    "\n",
    "data_types = np.tile(np.repeat([f\"Stimulus {j+1}\" for j in range(n_stimuli)], n_samples_per_stimulus), n_subjects)\n",
    "x = embedding_2d[:,0]\n",
    "y = embedding_2d[:,1]\n",
    "df = pd.DataFrame({\n",
    "    'Subject': subjects,\n",
    "    'Data Type': data_types,\n",
    "    'X': x,\n",
    "    'Y': y\n",
    "})\n",
    "\n",
    "# palette = sns.color_palette(\"hsv\", 10) \n",
    "markers = ['o', 's', 'D']\n",
    "\n",
    "for (data_type, marker) in zip(df['Data Type'].unique(), markers):\n",
    "    subset = df[df['Data Type'] == data_type]\n",
    "    sns.scatterplot(data=subset, x='X', y='Y', hue='Subject',style='Data Type', markers={data_type: marker}, s=10,legend=False)\n",
    "    \n",
    "plt.xlabel('UMAP 1', fontsize=22)\n",
    "plt.ylabel('UMAP 2', fontsize=22)\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'./figures/subject_stimuli.pdf', format='pdf', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 8),dpi=800)\n",
    "\n",
    "all_v = np.array(all_list)\n",
    "\n",
    "all_v = (all_v -np.min(all_v)+1e-5)/(np.max(all_v )-np.min(all_v )+1e-5)\n",
    "\n",
    "data_groups = {subject:norm_list for subject,norm_list in zip(subject_list,all_v)}\n",
    "df = pd.DataFrame(data_groups)\n",
    "ax = sns.kdeplot(df, shade=True)\n",
    "plt.xlabel('Variability Value', fontsize=22)\n",
    "plt.ylabel('Density', fontsize=22)\n",
    "plt.xticks(fontsize=16) \n",
    "plt.yticks(fontsize=16) \n",
    "plt.xlim(-0.1, 0.7)\n",
    "# plt.ylim(0, 0.5)\n",
    "plt.setp(ax.get_legend().get_texts(), fontsize=25)\n",
    "\n",
    "plt.tight_layout()\n",
    "save_path = 'figures/subject_variability.png'\n",
    "plt.savefig(save_path.replace('.png','.pdf'), format='pdf')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "making attention of type 'vanilla' with 512 in_channels\n",
      "Working with z of shape (1, 17, 62, 62) = 65348 dimensions.\n",
      "making attention of type 'vanilla' with 512 in_channels\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3862392/3950674974.py:16: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  ckpt= torch.load(ckpt_pth)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for AutoencoderKL:\n\tUnexpected key(s) in state_dict: \"loss.perceptual_loss.scaling_layer.shift\", \"loss.perceptual_loss.scaling_layer.scale\", \"loss.perceptual_loss.net.slice1.0.weight\", \"loss.perceptual_loss.net.slice1.0.bias\", \"loss.perceptual_loss.net.slice1.2.weight\", \"loss.perceptual_loss.net.slice1.2.bias\", \"loss.perceptual_loss.net.slice2.5.weight\", \"loss.perceptual_loss.net.slice2.5.bias\", \"loss.perceptual_loss.net.slice2.7.weight\", \"loss.perceptual_loss.net.slice2.7.bias\", \"loss.perceptual_loss.net.slice3.10.weight\", \"loss.perceptual_loss.net.slice3.10.bias\", \"loss.perceptual_loss.net.slice3.12.weight\", \"loss.perceptual_loss.net.slice3.12.bias\", \"loss.perceptual_loss.net.slice3.14.weight\", \"loss.perceptual_loss.net.slice3.14.bias\", \"loss.perceptual_loss.net.slice4.17.weight\", \"loss.perceptual_loss.net.slice4.17.bias\", \"loss.perceptual_loss.net.slice4.19.weight\", \"loss.perceptual_loss.net.slice4.19.bias\", \"loss.perceptual_loss.net.slice4.21.weight\", \"loss.perceptual_loss.net.slice4.21.bias\", \"loss.perceptual_loss.net.slice5.24.weight\", \"loss.perceptual_loss.net.slice5.24.bias\", \"loss.perceptual_loss.net.slice5.26.weight\", \"loss.perceptual_loss.net.slice5.26.bias\", \"loss.perceptual_loss.net.slice5.28.weight\", \"loss.perceptual_loss.net.slice5.28.bias\", \"loss.perceptual_loss.lin0.model.1.weight\", \"loss.perceptual_loss.lin1.model.1.weight\", \"loss.perceptual_loss.lin2.model.1.weight\", \"loss.perceptual_loss.lin3.model.1.weight\", \"loss.perceptual_loss.lin4.model.1.weight\", \"loss.discriminator.main.0.weight\", \"loss.discriminator.main.0.bias\", \"loss.discriminator.main.2.weight\", \"loss.discriminator.main.3.weight\", \"loss.discriminator.main.3.bias\", \"loss.discriminator.main.3.running_mean\", \"loss.discriminator.main.3.running_var\", \"loss.discriminator.main.3.num_batches_tracked\", \"loss.discriminator.main.5.weight\", \"loss.discriminator.main.6.weight\", \"loss.discriminator.main.6.bias\", \"loss.discriminator.main.6.running_mean\", \"loss.discriminator.main.6.running_var\", \"loss.discriminator.main.6.num_batches_tracked\", \"loss.discriminator.main.8.weight\", \"loss.discriminator.main.9.weight\", \"loss.discriminator.main.9.bias\", \"loss.discriminator.main.9.running_mean\", \"loss.discriminator.main.9.running_var\", \"loss.discriminator.main.9.num_batches_tracked\", \"loss.discriminator.main.11.weight\", \"loss.discriminator.main.11.bias\". ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 17\u001b[0m\n\u001b[1;32m     15\u001b[0m ckpt_pth \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/home/wht/multimodal_brain/src/tasks/1_eeg_pretrain/exp/VAE Pretrain/version_12/checkpoints/epoch=49-step=16200.ckpt\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     16\u001b[0m ckpt\u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(ckpt_pth)\n\u001b[0;32m---> 17\u001b[0m model\u001b[38;5;241m.\u001b[39mload_state_dict(ckpt[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstate_dict\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     19\u001b[0m data_config \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/home/wht/multimodal_brain/src/tasks/base/configs/pretrain.yaml\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     20\u001b[0m data_config \u001b[38;5;241m=\u001b[39m OmegaConf\u001b[38;5;241m.\u001b[39mload(data_config)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:2215\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[0;34m(self, state_dict, strict, assign)\u001b[0m\n\u001b[1;32m   2210\u001b[0m         error_msgs\u001b[38;5;241m.\u001b[39minsert(\n\u001b[1;32m   2211\u001b[0m             \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2212\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)))\n\u001b[1;32m   2214\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 2215\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2216\u001b[0m                        \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(error_msgs)))\n\u001b[1;32m   2217\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for AutoencoderKL:\n\tUnexpected key(s) in state_dict: \"loss.perceptual_loss.scaling_layer.shift\", \"loss.perceptual_loss.scaling_layer.scale\", \"loss.perceptual_loss.net.slice1.0.weight\", \"loss.perceptual_loss.net.slice1.0.bias\", \"loss.perceptual_loss.net.slice1.2.weight\", \"loss.perceptual_loss.net.slice1.2.bias\", \"loss.perceptual_loss.net.slice2.5.weight\", \"loss.perceptual_loss.net.slice2.5.bias\", \"loss.perceptual_loss.net.slice2.7.weight\", \"loss.perceptual_loss.net.slice2.7.bias\", \"loss.perceptual_loss.net.slice3.10.weight\", \"loss.perceptual_loss.net.slice3.10.bias\", \"loss.perceptual_loss.net.slice3.12.weight\", \"loss.perceptual_loss.net.slice3.12.bias\", \"loss.perceptual_loss.net.slice3.14.weight\", \"loss.perceptual_loss.net.slice3.14.bias\", \"loss.perceptual_loss.net.slice4.17.weight\", \"loss.perceptual_loss.net.slice4.17.bias\", \"loss.perceptual_loss.net.slice4.19.weight\", \"loss.perceptual_loss.net.slice4.19.bias\", \"loss.perceptual_loss.net.slice4.21.weight\", \"loss.perceptual_loss.net.slice4.21.bias\", \"loss.perceptual_loss.net.slice5.24.weight\", \"loss.perceptual_loss.net.slice5.24.bias\", \"loss.perceptual_loss.net.slice5.26.weight\", \"loss.perceptual_loss.net.slice5.26.bias\", \"loss.perceptual_loss.net.slice5.28.weight\", \"loss.perceptual_loss.net.slice5.28.bias\", \"loss.perceptual_loss.lin0.model.1.weight\", \"loss.perceptual_loss.lin1.model.1.weight\", \"loss.perceptual_loss.lin2.model.1.weight\", \"loss.perceptual_loss.lin3.model.1.weight\", \"loss.perceptual_loss.lin4.model.1.weight\", \"loss.discriminator.main.0.weight\", \"loss.discriminator.main.0.bias\", \"loss.discriminator.main.2.weight\", \"loss.discriminator.main.3.weight\", \"loss.discriminator.main.3.bias\", \"loss.discriminator.main.3.running_mean\", \"loss.discriminator.main.3.running_var\", \"loss.discriminator.main.3.num_batches_tracked\", \"loss.discriminator.main.5.weight\", \"loss.discriminator.main.6.weight\", \"loss.discriminator.main.6.bias\", \"loss.discriminator.main.6.running_mean\", \"loss.discriminator.main.6.running_var\", \"loss.discriminator.main.6.num_batches_tracked\", \"loss.discriminator.main.8.weight\", \"loss.discriminator.main.9.weight\", \"loss.discriminator.main.9.bias\", \"loss.discriminator.main.9.running_mean\", \"loss.discriminator.main.9.running_var\", \"loss.discriminator.main.9.num_batches_tracked\", \"loss.discriminator.main.11.weight\", \"loss.discriminator.main.11.bias\". "
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import sys,os,torch\n",
    "from omegaconf import OmegaConf\n",
    "import matplotlib.pyplot as plt\n",
    "device = torch.device('cuda:6' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '../')))\n",
    "from base.utils import instantiate_from_config\n",
    "from base.data import load_data\n",
    "import numpy as np\n",
    "config = '/home/wht/multimodal_brain/src/tasks/base/configs/autoencoder_kl_32x32x4.yaml'\n",
    "config = OmegaConf.load(config)\n",
    "model = instantiate_from_config(config['model'])\n",
    "model = model.to(device)\n",
    "\n",
    "ckpt_pth = '/home/wht/multimodal_brain/src/tasks/1_eeg_pretrain/exp/VAE Pretrain/version_12/checkpoints/epoch=49-step=16200.ckpt'\n",
    "ckpt= torch.load(ckpt_pth)\n",
    "model.load_state_dict(ckpt['state_dict'])\n",
    "\n",
    "data_config = '/home/wht/multimodal_brain/src/tasks/base/configs/pretrain.yaml'\n",
    "data_config = OmegaConf.load(data_config)\n",
    "data_config['data']['test_avg'] = False\n",
    "train_loader, test_loader = load_data(data_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eeg_latent = []\n",
    "labels = []\n",
    "# eegs = []\n",
    "for i,batch in enumerate(test_loader):\n",
    "    eeg, label, img, img_features, text, text_features, session, subject = batch\n",
    "    eeg =eeg.to(device)\n",
    "    dec, posterior, z = model(eeg)\n",
    "    # eegs.append(eeg.view(z.shape[0],-1).cpu().detach().numpy())\n",
    "    eeg_latent.append(z.view(z.shape[0],-1).cpu().detach().numpy())\n",
    "    labels.append(subject.cpu().detach().numpy())\n",
    "eeg_latent_v= np.concatenate(eeg_latent,axis=0)\n",
    "eeg_latent_v = eeg_latent_v.reshape(eeg_latent_v.shape[0],-1)\n",
    "\n",
    "# eegs = np.concatenate(eegs,axis=0)W\n",
    "# eegs_np = eegs.reshape(eegs.shape[0],-1)\n",
    "\n",
    "labels_np = np.array(labels)\n",
    "labels_np = labels_np.reshape(-1)\n",
    "\n",
    "print(eeg_latent_v.shape, labels_np.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import umap\n",
    "n_components = 100\n",
    "n_neighbors = 50\n",
    "pca = PCA(n_components=n_components)\n",
    "data_pca = pca.fit_transform(eeg_latent_v)\n",
    "reducer = umap.UMAP(n_neighbors=n_neighbors, random_state=0)\n",
    "embedding_2d = reducer.fit_transform(data_pca)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 8),dpi=800) \n",
    "subject_list = [f'sub-{str(i).zfill(2)}' for i in range(1, 11)]\n",
    "for label in np.unique(labels):\n",
    "    indices = labels_np == label\n",
    "    plt.scatter(embedding_2d[indices, 0][:500], embedding_2d[indices, 1][:500],label=subject_list[label],s=5)\n",
    "\n",
    "plt.xlabel('UMAP 1', fontsize=22)\n",
    "plt.ylabel('UMAP 2', fontsize=22)\n",
    "plt.xticks(fontsize=16) \n",
    "plt.yticks(fontsize=16) \n",
    "plt.legend(fontsize=15, markerscale=4.0)\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'./figures/subject_bias_avg{data_config['data']['test_avg']}.pdf', format='pdf', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import spearmanr,kendalltau,pearsonr\n",
    "var_list = []\n",
    "\n",
    "for i in range(10):\n",
    "    avg = sum(all_list[i]) / len(all_list[i])\n",
    "    var_list.append(avg)\n",
    "print(var_list)\n",
    "top1_acc = [6.1 ,4.9 , 5.6 , 5.0 , 4.0 , 6.0 , 6.5 , 8.8 , 4.3 , 7.0 ]\n",
    "correlation = np.corrcoef(var_list,top1_acc)[0, 1]\n",
    "print(correlation)\n",
    "\n",
    "my_top1_acc = [0.320 ,0.330, 0.355 ,0.390, 0.265, 0.320 ,0.400, 0.480 , 0.350 , 0.415 ] \n",
    "my_top1_acc = [100*ele for ele in my_top1_acc]\n",
    "correlation = np.corrcoef(var_list,my_top1_acc)[0, 1]\n",
    "print(correlation)\n",
    "\n",
    "\n",
    "top5_acc = [17.9 , 14.9, 17.4 , 15.1 , 13.4 , 18.2 , 20.4 , 23.7 , 14.0 , 19.7]\n",
    "correlation = np.corrcoef(var_list,top5_acc)[0, 1]\n",
    "print(correlation)\n",
    "\n",
    "\n",
    "\n",
    "def compute_coff(a,b):\n",
    "    corr_coefficient, p_value = pearsonr(a,b)\n",
    "    rho, p_value_rho = spearmanr(a,b)\n",
    "    tau, p_value_tau = kendalltau(a,b)\n",
    "    return corr_coefficient,rho,tau\n",
    "\n",
    "top1 = [6.11,4.9,5.58,4.96,4.01,6.01,6.51,8.79,4.34,7.04]\n",
    "top5 = [17.89,14.87,17.38,15.11,13.39,18.18,20.35,23.68,13.98,19.71]\n",
    "corr_coefficient,rho,tau = compute_coff(var_list, top1)\n",
    "print(f\"BraVL: top1: {corr_coefficient:.3f},{rho:.3f} {tau:.3f}\")\n",
    "corr_coefficient,rho,tau = compute_coff(var_list, top5)\n",
    "print(f\"BraVL: top5: {corr_coefficient:.3f},{rho:.3f} {tau:.3f}\")\n",
    "\n",
    "top1 = [12.3,10.4,13.1,16.4,8.0 , 14.1,  15.2,  20.0,  13.3,  14.9 ]\n",
    "top5 = [36.6,33.9,39.0,47.0,26.9, 40.6, 42.1, 49.9, 37.1, 41.9]\n",
    "corr_coefficient,rho,tau = compute_coff(var_list, top1)\n",
    "print(f\"NICE: top1: {corr_coefficient:.3f},{rho:.3f} {tau:.3f}\")\n",
    "corr_coefficient,rho,tau = compute_coff(var_list, top5)\n",
    "print(f\"NICE: top5: {corr_coefficient:.3f},{rho:.3f} {tau:.3f}\")\n",
    "\n",
    "\n",
    "top1 = [13.3 ,12.1 ,15.3, 15.9 ,9.8,  14.2 , 17.9,18.2 ,14.4 ,16.0] \n",
    "top5 = [40.2,36.1 ,39.6 ,49.0 ,34.4 ,42.4 ,43.6 ,50.2,38.7,42.8]\n",
    "corr_coefficient,rho,tau = compute_coff(var_list, top1)\n",
    "print(f\"NICE-SA: top1: {corr_coefficient:.3f},{rho:.3f} {tau:.3f}\")\n",
    "corr_coefficient,rho,tau = compute_coff(var_list, top5)\n",
    "print(f\"NICE-SA: top5: {corr_coefficient:.3f},{rho:.3f} {tau:.3f}\")\n",
    "\n",
    "\n",
    "\n",
    "top1 = [15.2 , 13.9 , 14.7 , 17.6 , 9.0 , 16.4 , 14.9 , 20.3 , 14.1 , 19.6] \n",
    "top5 = [40.1 ,40.1 ,42.7 ,48.9 ,29.7 ,44.4 ,43.1 , 52.1 , 39.7 ,46.7]\n",
    "corr_coefficient,rho,tau = compute_coff(var_list, top1)\n",
    "print(f\"NICE-GA: top1: {corr_coefficient:.3f},{rho:.3f} {tau:.3f}\")\n",
    "corr_coefficient,rho,tau = compute_coff(var_list, top5)\n",
    "print(f\"NICE-GA: top5: {corr_coefficient:.3f},{rho:.3f} {tau:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import kendalltau\n",
    "\n",
    "# 示例数据\n",
    "x = [1, 2, 3, 4, 5]\n",
    "y = [4, 1, 3, 5,2]\n",
    "y = [1, 2, 3, 4, 5]\n",
    "# 计算 Kendall’s Tau\n",
    "tau, p_value = kendalltau(x, y)\n",
    "\n",
    "print(\"Kendall’s Tau:\", tau)\n",
    "print(\"p-value:\", p_value)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
